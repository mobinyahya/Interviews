{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '/kaggle/input/transformersscript/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mobin/anaconda3/envs/sfusd-project/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import numpy as np\n",
    "# from timeseries_preprocessing import timeseries_dataset_from_array\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "file_path = '/Users/mobin/Documents/quant finance Interview/My interviews/Kaggle/'\n",
    "df1 = pd.read_csv(file_path+ \"household_power_consumption.txt\",delimiter=\";\")\n",
    "df1[\"datetime\"] = df1[[\"Date\",\"Time\"]].apply(lambda x: x[0]+\" \"+x[1],axis=1)\n",
    "df1[\"datetime\"] = pd.to_datetime(df1[\"datetime\"], infer_datetime_format=True)\n",
    "df1 = df1.drop([\"Date\",\"Time\"],axis=1)\n",
    "df1.index = df1[\"datetime\"]\n",
    "df1 = df1.drop(\"datetime\",axis=1)\n",
    "df1 = df1.apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Global_active_power  Global_reactive_power  Voltage  \\\n",
      "datetime                                                                   \n",
      "2006-12-16 17:24:00                4.216                  0.418   234.84   \n",
      "2006-12-16 17:25:00                5.360                  0.436   233.63   \n",
      "2006-12-16 17:26:00                5.374                  0.498   233.29   \n",
      "2006-12-16 17:27:00                5.388                  0.502   233.74   \n",
      "2006-12-16 17:28:00                3.666                  0.528   235.68   \n",
      "...                                  ...                    ...      ...   \n",
      "2010-11-26 20:58:00                0.946                  0.000   240.43   \n",
      "2010-11-26 20:59:00                0.944                  0.000   240.00   \n",
      "2010-11-26 21:00:00                0.938                  0.000   239.82   \n",
      "2010-11-26 21:01:00                0.934                  0.000   239.70   \n",
      "2010-11-26 21:02:00                0.932                  0.000   239.55   \n",
      "\n",
      "                     Global_intensity  Sub_metering_1  Sub_metering_2  \\\n",
      "datetime                                                                \n",
      "2006-12-16 17:24:00              18.4             0.0             1.0   \n",
      "2006-12-16 17:25:00              23.0             0.0             1.0   \n",
      "2006-12-16 17:26:00              23.0             0.0             2.0   \n",
      "2006-12-16 17:27:00              23.0             0.0             1.0   \n",
      "2006-12-16 17:28:00              15.8             0.0             1.0   \n",
      "...                               ...             ...             ...   \n",
      "2010-11-26 20:58:00               4.0             0.0             0.0   \n",
      "2010-11-26 20:59:00               4.0             0.0             0.0   \n",
      "2010-11-26 21:00:00               3.8             0.0             0.0   \n",
      "2010-11-26 21:01:00               3.8             0.0             0.0   \n",
      "2010-11-26 21:02:00               3.8             0.0             0.0   \n",
      "\n",
      "                     Sub_metering_3  \n",
      "datetime                             \n",
      "2006-12-16 17:24:00            17.0  \n",
      "2006-12-16 17:25:00            16.0  \n",
      "2006-12-16 17:26:00            17.0  \n",
      "2006-12-16 17:27:00            17.0  \n",
      "2006-12-16 17:28:00            17.0  \n",
      "...                             ...  \n",
      "2010-11-26 20:58:00             0.0  \n",
      "2010-11-26 20:59:00             0.0  \n",
      "2010-11-26 21:00:00             0.0  \n",
      "2010-11-26 21:01:00             0.0  \n",
      "2010-11-26 21:02:00             0.0  \n",
      "\n",
      "[2075259 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)\n",
    "df1 = df1.interpolate()\n",
    "df2 = df1.resample('30T').mean()\n",
    "df2 = df2.interpolate()\n",
    "df2_data = df2.values\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df2_data)\n",
    "df2_data = scaler.transform(df2_data)\n",
    "\n",
    "for i,name in enumerate(df2.columns):\n",
    "    df2[name] = df2_data[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 23:01:02.440660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2.iloc[:,:-3].values, df2.iloc[:,-3:].values, test_size=0.05,shuffle=False)\n",
    "data = tf.cast(X_train,tf.float32)\n",
    "targets = tf.cast(y_train,tf.float32)\n",
    "\n",
    "sample_length = 24\n",
    "input_dataset = timeseries_dataset_from_array(data,None, sequence_length=sample_length,batch_size=256, sequence_stride=sample_length)\n",
    "target_dataset = timeseries_dataset_from_array(targets, None, sequence_length=6,batch_size=256, sequence_stride=sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pz/v4b_yksj079_qtq_s0w50m9r0000gn/T/ipykernel_8580/356385997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# xformer.load_weights(\"xformer.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import Transformer\n",
    "xformer = Transformer(3,num_layers=4,num_heads=4,dff=256,d_model=32)\n",
    "pred = xformer(tf.random.normal((1,24,4)),tf.random.normal((1,6,3)),False)\n",
    "# xformer.load_weights(\"xformer.h5\")\n",
    "print(pred.shape)\n",
    "xformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU,Dense,Input,TimeDistributed,RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "in1 = Input((24,4))\n",
    "gru1 = GRU(128)(in1)\n",
    "gru2 = RepeatVector(6)(gru1)\n",
    "gru2 = GRU(128,return_sequences=True)(gru2)\n",
    "gru2 = TimeDistributed(Dense(3))(gru2)\n",
    "\n",
    "model = Model(in1,gru2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
    "optimizer2 = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
    "\n",
    "len_data = sum(1 for _ in input_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(200):\n",
    "    for i,batch in enumerate(zip(input_dataset, target_dataset)):\n",
    "\n",
    "        if i<len_data-1:\n",
    "            in1, tar = batch\n",
    "            \n",
    "            tar_inp = tf.ones((256,1,3))*-0.1\n",
    "            tar_inp = tf.concat([tar_inp,tar[:, :-1]],axis=1)\n",
    "            tar_real = tar[:, 1:]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = xformer(in1, tar_inp,True)\n",
    "                loss = tf.reduce_mean(tf.square(tar_real-predictions[:,:-1]))\n",
    "            gradients = tape.gradient(loss, xformer.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, xformer.trainable_variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                pred2 = model(in1)\n",
    "                loss2 = tf.reduce_mean(tf.square(tar-pred2))\n",
    "            gradients = tape.gradient(loss2, model.trainable_variables)\n",
    "            optimizer2.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                \n",
    "    print(f\"epoch {e}| Transformer:{loss.numpy()} | GRU:{loss2.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp):\n",
    "    \n",
    "    inp = inp.reshape(-1,24,4)\n",
    "#     inp = tf.reshape(inp,(-1,24,4))\n",
    "    n_batch = inp.shape[0]\n",
    "    start = np.ones((n_batch,1,3))*-0.1\n",
    "    \n",
    "    for i in range(6):\n",
    "        \n",
    "        pred = xformer(inp,start,False)\n",
    "        pred = np.expand_dims(pred[:,-1,:],0)\n",
    "        start = np.concatenate([start,pred],axis=1)\n",
    "#         print(start.shape)\n",
    "        \n",
    "    return start[:,1:]\n",
    "\n",
    "\n",
    "x_test = timeseries_dataset_from_array(X_test,None, sequence_length=sample_length,batch_size=1, sequence_stride=sample_length)\n",
    "y_test = timeseries_dataset_from_array(y_test, None, sequence_length=6,batch_size=1, sequence_stride=sample_length)\n",
    "x_test = list(x_test.as_numpy_iterator())\n",
    "y_test = list(y_test.as_numpy_iterator())\n",
    "x_test2 = np.asarray(x_test).reshape(-1,24,4)\n",
    "y_test2 = np.asarray(y_test).reshape(-1,6,3)\n",
    "preds_xform = [predict(x) for x in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_xform = np.mean([(y_true-y_pred)**2 for y_true,y_pred in zip(y_test2,preds_xform)])\n",
    "\n",
    "pred_gru = model(x_test2)\n",
    "err_gru = np.mean((pred_gru-y_test2)**2)\n",
    "\n",
    "err_xform,err_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfusd-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
