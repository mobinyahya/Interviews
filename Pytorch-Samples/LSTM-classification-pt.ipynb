{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# nlp library of Pytorch\n",
    "from torchtext import data\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "SEED = 2021\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cuda.deterministic = True\n",
    "\n",
    "file_path = '/Users/mobin/Documents/quant finance Interview/My interviews/Kaggle/'\n",
    "data_ = pd.read_csv(file_path+'/sms_spam.csv')\n",
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0.0\n",
      "Text: Should i send you naughty pix ? :)\n",
      "Length: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader  # For creating custom datasets and data loading\n",
    "import spacy  # Import spaCy for natural language processing tasks\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load a pre-trained English language model for tokenization\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [token.text for token in nlp(text)]  # Use spaCy to tokenize text into individual words\n",
    "\n",
    "class SMSDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # Store the input data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]['type'], self.data.iloc[idx]['text']  # Return a single sample (label and text)\n",
    "\n",
    "\n",
    "file_path = '/Users/mobin/Documents/quant finance Interview/My interviews/Kaggle/'\n",
    "data_ = pd.read_csv(file_path + '/sms_spam.csv')  \n",
    "dataset = SMSDataset(data_)  # Instantiate our custom dataset with the loaded data\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "def build_vocab(dataset, min_freq=2):\n",
    "    counter = Counter()  # Initialize a counter for word frequencies\n",
    "    for _, text in dataset:\n",
    "        counter.update(tokenizer(text))  # Count word occurrences across all texts\n",
    "    return {word: i+1 for i, (word, count) in enumerate(counter.items()) if count >= min_freq}  # Create word-to-index mapping\n",
    "\n",
    "vocab = build_vocab(dataset)  # Build the vocabulary from our dataset\n",
    "vocab['<unk>'] = 0  # Add an 'unknown' token to handle words not in the vocabulary\n",
    "\n",
    "# Define text pipeline\n",
    "text_pipeline = lambda x: [vocab.get(word, vocab['<unk>']) for word in tokenizer(x)]  # Convert text to sequence of indices\n",
    "label_pipeline = lambda x: 1 if x.lower() == 'spam' else 0  # Convert label to binary (1 for spam, 0 for ham)\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))  # Process labels\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # Convert text to tensor of indices\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))  # Store length of each sequence\n",
    "    label_list = torch.tensor(label_list)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True)  # Pad sequences to same length\n",
    "    return label_list.float(), text_list, torch.tensor(lengths)\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)  # Create DataLoader for efficient batching\n",
    "\n",
    "# Print an example\n",
    "for label, text, lengths in train_dataloader:\n",
    "    print(f\"Label: {label[0].item()}\")  # Print the label of the first item in the batch\n",
    "    print(f\"Text: {' '.join([list(vocab.keys())[list(vocab.values()).index(i)] for i in text[0] if i != 0])}\")  # Reconstruct and print the text\n",
    "    print(f\"Length: {lengths[0].item()}\")  # Print the length of the first sequence\n",
    "    break  # Only print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch's nn module has lots of useful feature\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "        \n",
    "        super(LSTMNet,self).__init__()\n",
    "        \n",
    "        # Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        \n",
    "        # LSTM layer process the vector sequences \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True\n",
    "                           )\n",
    "        \n",
    "        # Dense layer to predict \n",
    "        self.fc = nn.Linear(hidden_dim * 2,output_dim)\n",
    "        # Prediction activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Thanks to packing, LSTM don't see padding tokens \n",
    "        # and this makes our model better\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\n",
    "        \n",
    "        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Concatenating the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        \n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.sigmoid(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pz/v4b_yksj079_qtq_s0w50m9r0000gn/T/ipykernel_14003/3737390569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSIZE_OF_VOCAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mNUM_HIDDEN_NODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNUM_OUTPUT_NODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_HIDDEN_NODES = 64\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.2\n",
    "model = LSTMNet(SIZE_OF_VOCAB,\n",
    "                EMBEDDING_DIM,\n",
    "                NUM_HIDDEN_NODES,\n",
    "                NUM_OUTPUT_NODES,\n",
    "                NUM_LAYERS,\n",
    "                BIDIRECTION,\n",
    "                DROPOUT\n",
    "               )\n",
    "import torch.optim as optim\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfusd-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
